# Docker Compose é…ç½® - LLM Data Lab

services:
  # åç«¯æœåŠ¡
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: llm-data-lab-backend
    ports:
      - "8000:8000"
    volumes:
      # æŒä¹…åŒ–æ•°æ®å­˜å‚¨
      - ./uploaded_datasets:/app/uploaded_datasets
      - ./analysis_artifacts:/app/analysis_artifacts
      - backend-db:/app/db
    environment:
      # ä» .env æ–‡ä»¶è¯»å–ç¯å¢ƒå˜é‡
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-change-this-to-a-32-character-secret-key-in-production}
      - JWT_ALGORITHM=${JWT_ALGORITHM:-HS256}
      - ACCESS_TOKEN_EXPIRES_MINUTES=${ACCESS_TOKEN_EXPIRES_MINUTES:-43200}
      - CREDENTIALS_SECRET_KEY=${CREDENTIALS_SECRET_KEY:-}
      - DATABASE_URL=${DATABASE_URL:-sqlite+aiosqlite:///./db/llm_data_lab.db}
      # LLM API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_DEFAULT_MODELS=${OPENAI_DEFAULT_MODELS:-["gpt-4o","gpt-4o-mini","gpt-4.1"]}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - DEEPSEEK_BASE_URL=${DEEPSEEK_BASE_URL:-https://api.deepseek.com}
      - DEEPSEEK_DEFAULT_MODELS=${DEEPSEEK_DEFAULT_MODELS:-["deepseek-chat","deepseek-coder"]}
      - DASHSCOPE_API_KEY=${DASHSCOPE_API_KEY:-}
      - DASHSCOPE_BASE_URL=${DASHSCOPE_BASE_URL:-https://dashscope.aliyuncs.com}
      - QWEN_DEFAULT_MODELS=${QWEN_DEFAULT_MODELS:-["qwen-turbo","qwen-plus","qwen-max"]}
      - SILICONFLOW_API_KEY=${SILICONFLOW_API_KEY:-}
      - SILICONFLOW_BASE_URL=${SILICONFLOW_BASE_URL:-https://api.siliconflow.cn}
      - SILICONFLOW_DEFAULT_MODELS=${SILICONFLOW_DEFAULT_MODELS:-["siliconflow-chat","siliconflow-coder"]}
      # æ‰§è¡Œé™åˆ¶
      - MAX_CODE_EXECUTION_SECONDS=${MAX_CODE_EXECUTION_SECONDS:-60}
      - MAX_CODE_EXECUTION_MEMORY_MB=${MAX_CODE_EXECUTION_MEMORY_MB:-768}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - llm-data-lab-network

  # å‰ç«¯æœåŠ¡
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: llm-data-lab-frontend
    ports:
      - "3000:3000"
    environment:
      # ğŸŒ ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨åŸŸåè®¿é—®åç«¯ APIï¼ˆé€šè¿‡ Nginx /api/ è·¯å¾„ï¼‰
      # ğŸ“ éƒ¨ç½²åˆ°è‡ªå·±çš„åŸŸåæ—¶ï¼Œå°† btchuro.com æ”¹ä¸ºä½ çš„åŸŸå
      - NEXT_PUBLIC_API_BASE_URL=${NEXT_PUBLIC_API_BASE_URL:-http://localhost:8000}
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - llm-data-lab-network

# ç½‘ç»œé…ç½®
networks:
  llm-data-lab-network:
    driver: bridge

# æ•°æ®å·
volumes:
  backend-db:
    driver: local
